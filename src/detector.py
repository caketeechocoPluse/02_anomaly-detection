# src/detector.py


import pandas as pd
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from datetime import datetime


class AnomalyDetector:
    """ì´ìƒ ê±°ë˜ íƒì§€ê¸°"""

    def __init__(self, df):
        self.df = df.copy()
        self.anomalies = []

    def detect_all(self):
        """ëª¨ë“  íƒì§€ ê¸°ë²• ì‹¤í–‰"""
        print("ğŸ” ì´ìƒ ê±°ë˜ íƒì§€ ì‹œì‘...")

        self.detect_duplicates()
        self.detect_round_amounts()
        self.detect_weekend_transactions()
        self.detect_statistical_outliers()
        self.detect_benford_law_violation()
        self.detect_frequent_small_transactions()

        return self.get_results()

    def detect_duplicates(self):
        """ì¤‘ë³µ ê±°ë˜ íƒì§€"""
        print("  - ì¤‘ë³µ ê±°ë˜ íƒì§€ ì¤‘...")

        # ë™ì¼ ë‚ ì§œ, ê³„ì •, ê¸ˆì•¡, ê±°ë˜ì²˜
        duplicates = self.df[
            self.df.duplicated(
                subset=["ê±°ë˜ì¼ì", "ê³„ì •ê³¼ëª©", "ê¸ˆì•¡", "ê±°ë˜ì²˜"], keep=False
            )
        ]

        for idx in duplicates.index:
            self.anomalies.append(
                {
                    "index": idx,
                    "type": "ì¤‘ë³µê±°ë˜",
                    "severity": "HIGH",
                    "description": "ë™ì¼í•œ ê±°ë˜ê°€ ì¤‘ë³µ ë°œìƒ",
                    "score": 0.9,
                }
            )

    def detect_round_amounts(self):
        """ë¼ìš´ë“œ ê¸ˆì•¡ íƒì§€"""
        print("  - ë¼ìš´ë“œ ê¸ˆì•¡ íƒì§€ ì¤‘...")

        # ë°±ë§Œì› ë‹¨ìœ„ ë¼ìš´ë“œ
        round_amounts = self.df[
            (self.df["ê¸ˆì•¡"] % 1000000 == 0) & (self.df["ê¸ˆì•¡"] >= 1000000)
        ]

        for idx in round_amounts.index:
            self.anomalies.append(
                {
                    "index": idx,
                    "type": "ë¼ìš´ë“œê¸ˆì•¡",
                    "severity": "MEDIUM",
                    "description": f"{self.df.loc[idx, 'ê¸ˆì•¡']:,}ì› (ë°±ë§Œì› ë‹¨ìœ„)",
                    "score": 0.6,
                }
            )

    def detect_weekend_transactions(self):
        """ì£¼ë§ ê±°ë˜ íƒì§€"""
        print("  - ì£¼ë§ ê±°ë˜ íƒì§€ ì¤‘...")

        self.df["ìš”ì¼"] = pd.to_datetime(self.df["ê±°ë˜ì¼ì"]).dt.dayofweek
        weekend = self.df[self.df["ìš”ì¼"] >= 5]  # í† (5), ì¼(6)

        for idx in weekend.index:
            self.anomalies.append(
                {
                    "index": idx,
                    "type": "ì£¼ë§ê±°ë˜",
                    "severity": "MEDIUM",
                    "description": "ì£¼ë§ì— ë°œìƒí•œ ê±°ë˜",
                    "score": 0.7,
                }
            )

    def detect_statistical_outliers(self):
        """í†µê³„ì  ì´ìƒì¹˜ íƒì§€ (IQR, Z-score)"""
        print("  - í†µê³„ì  ì´ìƒì¹˜ íƒì§€ ì¤‘...")

        for account in self.df["ê³„ì •ê³¼ëª©"].unique():
            account_df = self.df[self.df["ê³„ì •ê³¼ëª©"] == account]
            amounts = account_df["ê¸ˆì•¡"]

            # IQR ë°©ì‹
            Q1 = amounts.quantile(0.25)
            Q3 = amounts.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = account_df[
                (account_df["ê¸ˆì•¡"] < lower_bound) | (account_df["ê¸ˆì•¡"] > upper_bound)
            ]

            for idx in outliers.index:
                z_score = abs(stats.zscore(amounts)[account_df.index.get_loc(idx)])
                self.anomalies.append(
                    {
                        "index": idx,
                        "type": "í†µê³„ì ì´ìƒì¹˜",
                        "severity": "HIGH" if z_score > 3 else "MEDIUM",
                        "description": f"Z-score: {z_score:.2f}",
                        "score": min(z_score / 3, 1.0),
                    }
                )

    def detect_benford_law_violation(self):
        """ë²¤í¬ë“œ ë²•ì¹™ ìœ„ë°˜ íƒì§€"""
        print("  - ë²¤í¬ë“œ ë²•ì¹™ ê²€ì¦ ì¤‘...")

        # ì²« ìë¦¬ ìˆ«ì ì¶”ì¶œ
        first_digits = self.df["ê¸ˆì•¡"].astype(str).str[0].astype(int)

        # ë²¤í¬ë“œ ë²•ì¹™ ê¸°ëŒ€ ë¶„í¬
        benford_dist = {
            1: 0.301,
            2: 0.176,
            3: 0.125,
            4: 0.097,
            5: 0.079,
            6: 0.067,
            7: 0.058,
            8: 0.051,
            9: 0.046,
        }

        # ì‹¤ì œ ë¶„í¬
        actual_dist = first_digits.value_counts(normalize=True).to_dict()

        # Chi-square ê²€ì •
        for digit in range(1, 10):
            expected = benford_dist[digit] * len(self.df)
            actual = (first_digits == digit).sum()

            # í¸ì°¨ê°€ í° ê²½ìš°
            if abs(actual - expected) / expected > 0.5:
                suspicious = self.df[first_digits == digit]
                for idx in suspicious.index[:10]:  # ìƒìœ„ 10ê°œë§Œ
                    self.anomalies.append(
                        {
                            "index": idx,
                            "type": "ë²¤í¬ë“œë²•ì¹™ìœ„ë°˜",
                            "severity": "LOW",
                            "description": f"ì²« ìë¦¬ {digit} ë¹ˆë„ ì´ìƒ",
                            "score": 0.4,
                        }
                    )

    def detect_frequent_small_transactions(self):
        """ë¹ˆë²ˆí•œ ì†Œì•¡ ê±°ë˜ íƒì§€ (ë¶„í•  ì˜ì‹¬)"""
        print("  - ë¹ˆë²ˆí•œ ì†Œì•¡ ê±°ë˜ íƒì§€ ì¤‘...")

        # ë‹´ë‹¹ìë³„ ì†Œì•¡ ê±°ë˜ ë¹ˆë„
        threshold = 100000  # 10ë§Œì›
        small_txns = self.df[self.df["ê¸ˆì•¡"] < threshold]

        freq = small_txns.groupby("ë‹´ë‹¹ì").size()
        suspicious_users = freq[freq > freq.quantile(0.95)].index

        for user in suspicious_users:
            user_txns = small_txns[small_txns["ë‹´ë‹¹ì"] == user]
            for idx in user_txns.index:
                self.anomalies.append(
                    {
                        "index": idx,
                        "type": "ë¹ˆë²ˆí•œì†Œì•¡ê±°ë˜",
                        "severity": "MEDIUM",
                        "description": f"{user} - ì†Œì•¡ ê±°ë˜ {len(user_txns)}ê±´",
                        "score": 0.65,
                    }
                )

    def get_results(self):
        """íƒì§€ ê²°ê³¼ ë°˜í™˜"""
        if not self.anomalies:
            return pd.DataFrame()

        # ì¤‘ë³µ ì œê±° (ê°™ì€ ê±°ë˜ì— ì—¬ëŸ¬ ì´ìƒ ìœ í˜•)
        anomaly_df = pd.DataFrame(self.anomalies)
        anomaly_df = anomaly_df.sort_values("score", ascending=False)

        # ì›ë³¸ ë°ì´í„°ì™€ ì¡°ì¸
        result = self.df.loc[anomaly_df["index"]].copy()
        result["íƒì§€ìœ í˜•"] = anomaly_df["type"].values
        result["ì‹¬ê°ë„"] = anomaly_df["severity"].values
        result["ì„¤ëª…"] = anomaly_df["description"].values
        result["ìœ„í—˜ì ìˆ˜"] = anomaly_df["score"].values

        print(f"\\nâœ… íƒì§€ ì™„ë£Œ: {len(result)}ê±´ì˜ ì˜ì‹¬ ê±°ë˜ ë°œê²¬")
        return result


# ì‹¤í–‰
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv("data/raw/transactions.csv")

    # íƒì§€ ì‹¤í–‰
    detector = AnomalyDetector(df)
    anomalies = detector.detect_all()

    # ê²°ê³¼ ì €ì¥
    anomalies.to_csv(
        "outputs/detected_anomalies.csv", index=False, encoding="utf-8-sig"
    )

    # ìš”ì•½ ì¶œë ¥
    print("\\nğŸ“Š íƒì§€ ê²°ê³¼ ìš”ì•½:")
    print(anomalies["íƒì§€ìœ í˜•"].value_counts())
    print(f"\\nì‹¬ê°ë„ë³„:")
    print(anomalies["ì‹¬ê°ë„"].value_counts())
